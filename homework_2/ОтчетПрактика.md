# Цель работы
Изучение и практическая реализация алгоритмов линейной и логистической регрессии с использованием градиентного спуска, включая математические основы и сравнение с реализацией в библиотеке scikit-learn.

# Выполненные задания
**Линейная регрессия**
Генерация данных
Сгенерирован синтетический набор данных с линейной зависимостью: y = 4x + 3 + ε

Добавлен случайный шум для приближения к реальным условиям

Данные разделены на обучающую (80%) и тестовую (20%) выборки

Реализованные функции:
*Функция предсказания predict()*

python
def predict(X, w, b):
    return X @ w + b
Реализует линейную модель: ŷ = Xw + b

Протестирована на контрольных данных

*Функция потерь MSE mse()*

python
def mse(y, y_pred):
    return np.mean((y - y_pred) ** 2)
Вычисляет среднеквадратичную ошибку

Успешно протестирована

*Вычисление градиентов gradients()*

python
def gradients(X, y, w, b):
    n = len(y)
    y_pred = predict(X, w, b)
    dw = (-2/n) * X.T @ (y - y_pred)
    db = (-2/n) * np.sum(y - y_pred)
    return dw, db
Вычисляет градиенты по весам и смещению

Проверено направление градиента

*Градиентный спуск*

Реализован полный цикл обучения

Параметры: learning rate = 0.1, epochs = 1000

Получены параметры: w = 3.948, b = 3.029 (близко к истинным значениям)

Визуализация и сравнение
Построен график сходимости функции потерь

Сравнение с LinearRegression из scikit-learn:

Наша MSE: 0.2485119936374279

Sklearn MSE: 0.2485119936374281

Результаты практически идентичны

**Логистическая регрессия**
Теоретическая часть
Изучены математические основы:

Сигмоидная функция для преобразования в вероятности

Функция потерь LogLoss (отрицательное лог-правдоподобие)

Градиенты для логистической регрессии

Реализованные функции:
*Сигмоидная функция sigmoid()*

python
def sigmoid(z):
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))
*Предсказание вероятностей predict_proba()*

python
def predict_proba(X, w, b):
    z = X @ w + b
    return sigmoid(z)
*Функция потерь LogLoss logloss()*

python
def logloss(X, y, w, b, l2=0.0):
    p = predict_proba(X, w, b)
    p = np.clip(p, 1e-9, 1 - 1e-9)
    n = len(y)
    base = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))
    reg = l2 * np.sum(w**2)
    return base + reg
*Градиенты для логистической регрессии*

python
def gradients(X, y, w, b, l2=0.0):
    n = len(y)
    p = predict_proba(X, w, b)
    dw = (X.T @ (p - y)) / n + 2 * l2 * w
    db = np.mean(p - y)
    return dw, db
Обучение модели
Использованы стандартизированные данные

Параметры: lr = 0.1, epochs = 1000, l2 = 0.0

Final LogLoss: 0.2373

Построен график сходимости функции потерь

Ключевые результаты
Линейная регрессия
✅ Успешно реализован полный pipeline линейной регрессии

✅ Алгоритм корректно обучается и сходится

✅ Результаты идентичны реализации в scikit-learn

✅ Визуализирован процесс обучения

Логистическая регрессия
✅ Понимание математических основ (сигмоида, LogLoss)

✅ Корректная реализация всех компонентов

✅ Стабильная сходимость алгоритма

✅ Учет численной стабильности (clipping)

**Выводы**
Работа успешно демонстрирует:

Глубокое понимание математических основ регрессионных моделей

Практические навыки реализации алгоритмов машинного обучения с нуля

Умение работать с градиентным спуском и функциями потерь

Способность сравнивать собственные реализации с промышленными библиотеками

Навыки визуализации и анализа процесса обучения

Реализованные алгоритмы работают корректно и могут быть использованы как основа для более сложных моделей машинного обучения.