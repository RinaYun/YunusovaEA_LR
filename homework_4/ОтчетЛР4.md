# Отчет по лабораторной работе 4: Деревья. Случайный лес
**Общее описание работы**
В данной лабораторной работе были реализованы и протестированы алгоритмы, связанные с решающими деревьями и случайным лесом. Работа разделена на две основные части:

Основы построения решающего дерева

Реализация и тестирование собственной версии решающего дерева

***Часть 1: Основы построения решающего дерева***
**Задание 1.1: Расчет энтропии**
Цель: Рассчитать энтропию распределения классов с натуральным логарифмом для 10 объектов (8 класса k₁ и 2 класса k₂).

Реализация:

Использована формула энтропии: H = -Σ(pᵢ * log(pᵢ))

Рассчитаны вероятности: p₁ = 8/10 = 0.8, p₂ = 2/10 = 0.2

Результат: 0.50 (округлено до двух знаков)

Вывод: Энтропия распределения составляет 0.50, что указывает на умеренную неопределенность.

**Задание 1.2: Критерий информативности (Джини)**
Цель: Рассчитать критерий информативности Q с использованием индекса Джини.

Реализация:

Индекс Джини: H(R) = Σpₖ(1-pₖ)

Для родительской вершины: H(Rm) = 0.8×0.2 + 0.2×0.8 = 0.32

Для левого поддерева (только k₁): H(Rl) = 0.0

Для правого поддерева (только k₂): H(Rr) = 0.0

Критерий информативности: Q = 0.32 - (0.8×0.0) - (0.2×0.0) = 0.32

Вывод: Максимальное значение критерия (0.32) достигается при идеальном разделении классов.

**Задание 1.3: Предсказание в регрессии**
Цель: Определить предсказание модели для листовой вершины с 10 объектами в задаче регрессии.

Реализация:

Значения целевой переменной: [1, 10, 5, 18, 100, 30, 50, 61, 84, 47]

Предсказание вычислено как среднее арифметическое: 40.6

Вывод: В задачах регрессии решающее дерево предсказывает среднее значение целевой переменной в листовой вершине.

***Часть 2: Реализация решающего дерева***
**Задание 2.1: Функция find_best_split()**
Цель: Реализовать функцию для нахождения оптимального разбиения подмножества обучающей выборки.

Особенности реализации:

Векторизованная реализация (без циклов для вещественных признаков) - выполнено требование бонусного задания

Поддержка двух типов задач:

Классификация (критерий Джини)

Регрессия (дисперсия)

Поддержка двух типов признаков:

Вещественные

Категориальные

Ключевые алгоритмические решения:

Для вещественных признаков пороги рассчитываются как средние между соседними уникальными значениями

Для категориальных признаков используется наивный алгоритм разбиения по одному значению

Учтены краевые случаи (пустые поддеревья, константные признаки)

Обеспечена стабильность при одинаковых значениях критерия

Тестирование функции:

Классификация с вещественным признаком:

Найдено 4 порога с максимальным критерием 0.48 при пороге 2.5

Регрессия с вещественным признаком:

Лучший порог 3.5 с критерием 6.0

Производительность на большом наборе данных:

1000 samples обработаны за 9.05 ms

Найдено 999 порогов

Тестирование на датасете California Housing
Анализ признака MedInc:

Найдено 12927 порогов

Лучший порог: 5.0351

Лучшее значение критерия: 0.4128

Визуализация:
Построен график зависимости значения критерия ошибки от порогового значения при разбиении по признаку MedInc. График показывает плавное изменение критерия с четко выраженным максимумом.

Технические особенности реализации
Оптимизации:
Векторизация вычислений для вещественных признаков

Использование кумулятивных сумм для эффективного расчета статистик

One-hot encoding для быстрых расчетов в классификации

Оптимизированная работа с памятью через маски и индексацию

Обработка особых случаев:
Константные признаки

Пустые поддеревья

NaN значения

Одинаковые значения критерия

# Выводы
Теоретическая часть: Успешно применены основные формулы и концепции решающих деревьев (энтропия, индекс Джини, критерий информативности).

Практическая реализация: Создана эффективная векторная реализация функции find_best_split(), способная работать с большими объемами данных.

Качество кода: Реализация соответствует требованиям задания, включает обработку краевых случаев и оптимизирована для производительности.

Верификация: Функция протестирована на синтетических данных и реальном датасете, показала корректную работу и хорошую производительность.

Работа демонстрирует глубокое понимание алгоритмов решающих деревьев и умение создавать эффективные реализации машинного обучения с использованием библиотек NumPy и pandas.